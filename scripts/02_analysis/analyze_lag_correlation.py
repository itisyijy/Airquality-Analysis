# -*- coding: utf-8 -*-

import pandas as pd
from scipy.stats import pearsonr # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°ìš©
import os
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import sys

# --- ë°ì´í„° ê²½ë¡œ ì„¤ì • (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ì¤€) ---
# NOTE: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” scripts/02_analysis/ í´ë”ì— ìœ„ì¹˜í•˜ë©°, ë‘ ë‹¨ê³„ ìƒìœ„ë¡œ ì´ë™í•˜ì—¬ PROJECT_ROOTë¥¼ ì°¾ìŠµë‹ˆë‹¤.
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) 
PROJECT_ROOT_DIR = os.path.join(SCRIPT_DIR, '..', '..') # BDP-Airquality-Analysis ë£¨íŠ¸ í´ë”

# ì¶œë ¥ í´ë” ê²½ë¡œ (í†µí•©ëœ CSV íŒŒì¼ì´ ì €ì¥ëœ ìœ„ì¹˜)
OUTPUT_DIR = os.path.join(PROJECT_ROOT_DIR, 'results', 'pandas_analysis')
# ì…ë ¥: ìµœì¢… í†µí•© ë°ì´í„°ì…‹ (Merge Scriptì˜ ì¶œë ¥ CSV íŒŒì¼)
UNIFIED_MERGED_CSV = os.path.join(OUTPUT_DIR, "unified_national_merged_data.csv")

# ì¶œë ¥: ì‹œê°„ì°¨ ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
OUTPUT_LOCAL_LAG_CORR = os.path.join(OUTPUT_DIR, "lagged_correlation_results_pm10.csv")

# --- ë¶„ì„ ëŒ€ìƒ ì„¤ì • ---
TARGET_POLLUTANT = 'national_avg_PM10'   # ì¢…ì† ë³€ìˆ˜
PREDICTOR = 'Power_GWh'                  # ì˜ˆì¸¡ ë³€ìˆ˜


def analyze_lag_correlation():
    
    # 1. í†µí•© ë°ì´í„° ë¡œë“œ
    print("=== 1. í†µí•© CSV ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
    try:
        # í†µí•©ëœ CSV íŒŒì¼ì€ Date ì¸ë±ìŠ¤ë¥¼ ê°€ì§€ê³  ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ index_col=0 ì‚¬ìš©
        df_pandas = pd.read_csv(UNIFIED_MERGED_CSV, index_col=0, parse_dates=True)
        df_pandas.sort_index(inplace=True)
        print(f"-> ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ {len(df_pandas)}ê°œì›”ì˜ ë°ì´í„°.")
        
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: í†µí•© ë°ì´í„° íŒŒì¼({UNIFIED_MERGED_CSV})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("      'unified_analysis_parquet.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ì´ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        return

    # 2. ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼ ì„¤ì • ë° ìœ íš¨ì„± ê²€ì‚¬
    df_analysis = df_pandas[[TARGET_POLLUTANT, PREDICTOR]].astype(float) 
    
    lag_results = []
    
    # 3. ì„ í–‰ ì§€ì—° ìƒê´€ê´€ê³„ ê³„ì‚° (Lagged Correlation)
    # Lag 1ê°œì›”ë¶€í„° 6ê°œì›”ê¹Œì§€ ë¶„ì„ (í™˜ê²½ ì˜í–¥ì˜ ì‹œê°„ì°¨ë¥¼ ì°¾ê¸° ìœ„í•¨)
    print("\n=== 2. ë°œì „ëŸ‰(Power)ì˜ ì‹œê°„ì°¨(Lag) ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘ ===")
    
    for lag in range(1, 7):
        # Lag ì»¬ëŸ¼ ìƒì„±: ë°œì „ëŸ‰ ë°ì´í„°ë¥¼ lag ê¸°ê°„ë§Œí¼ ì•„ë˜ë¡œ ë°€ê¸° (shift)
        lag_col_name = f'{PREDICTOR}_Lag_{lag}'
        df_analysis[lag_col_name] = df_analysis[PREDICTOR].shift(lag)
        
        # Nanì´ ì—†ëŠ” ìœ íš¨í•œ ê°’ë§Œ ì„ íƒí•˜ì—¬ ë°°ì—´ ì¤€ë¹„
        valid_data = df_analysis[[TARGET_POLLUTANT, lag_col_name]].dropna()
        
        if len(valid_data) > 5: # ìµœì†Œ 5ê°œ ì´ìƒì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ ìˆì–´ì•¼ ë¶„ì„ ê°€ëŠ¥
            correlation, p_value = pearsonr(
                valid_data[TARGET_POLLUTANT],
                valid_data[lag_col_name]
            )
            
            lag_results.append({
                'Lag': lag,
                'Correlation_Coefficient': correlation,
                'P_Value': p_value
            })
            
            print(f"  - Lag {lag}ê°œì›”: ìƒê´€ê³„ìˆ˜ = {correlation:.4f}, P-ê°’ = {p_value:.4f}")
        else:
            print(f"  - Lag {lag}ê°œì›”: ë°ì´í„° í¬ì¸íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€.")


    # 4. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    df_lag_results = pd.DataFrame(lag_results)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    # ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥
    df_lag_results.to_csv(OUTPUT_LOCAL_LAG_CORR, index=False)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ CSV íŒŒì¼ì´ ë¡œì»¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {OUTPUT_LOCAL_LAG_CORR}")
    
    # ìµœì  Lag ì‹œê°ì  ì¶œë ¥
    if not df_lag_results.empty:
        best_lag = df_lag_results.loc[df_lag_results['Correlation_Coefficient'].abs().idxmax()]
        print(f"\nğŸ’¡ [ìµœì  Lag ë¶„ì„ ê²°ê³¼]: ìƒê´€ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì‹œê°„ì°¨ëŠ” {int(best_lag['Lag'])}ê°œì›”ì…ë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {best_lag['Correlation_Coefficient']:.4f}).")


if __name__ == "__main__":
    analyze_lag_correlation()